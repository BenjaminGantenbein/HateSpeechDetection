{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly transformers sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "df=pd.read_csv(\"Data_Example/train.csv\")\n",
    "df2=pd.read_csv(\"Data_Example/dev.csv\")\n",
    "df = pd.concat([df, df2], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_mentions(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "# Apply the function to your column\n",
    "df['text'] = df['text'].apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df, target_col):\n",
    "    \"\"\"\n",
    "    Balances a dataset by randomly removing instances from overrepresented classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - target_col: The column name of the target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - A balanced DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of instances for each class\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    \n",
    "    # Find the smallest class size\n",
    "    min_class_size = class_counts.min()\n",
    "    \n",
    "    # For each class, randomly sample instances to match the size of the smallest class\n",
    "    balanced_dfs = [df[df[target_col] == class_label].sample(n=min_class_size, random_state=42) \n",
    "                   for class_label in class_counts.index]\n",
    "    \n",
    "    # Concatenate the dataframes\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['label']==1]))\n",
    "print(len(df[df['label']==0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = balance_dataset(df, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['label']==1]))\n",
    "print(len(df[df['label']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, stratify=df['label'])\n",
    "#train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])\n",
    "#val_dataset = Dataset.from_pandas(val_df)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "#full_dataset = Dataset.from_pandas(df)\n",
    "dataset = {'train': train_dataset,'test': test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-german-cased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for key in dataset:\n",
    "    tokenized_datasets[key] = dataset[key].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "from sklearn.metrics import f1_score\n",
    "import tempfile\n",
    "\n",
    "checkpoint = \"distilbert-base-german-cased\"\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "config.dropout = 0.5          \n",
    "config.attention_dropout = 0.5\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    config.save_pretrained(temp_dir)\n",
    "    \n",
    "    # Also save the model weights to the temp directory\n",
    "    model_weights = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    model_weights.save_pretrained(temp_dir)\n",
    "    \n",
    "    # Now load the model with the modified config from the temp directory\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(temp_dir, num_labels=2)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions)  # Calculate the F1 score first\n",
    "    acc_metrics = metric.compute(predictions=predictions, references=labels)  # This should give you accuracy.\n",
    "    \n",
    "    return {\"f1\": f1, **acc_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoConfig, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define search space\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.4)\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "    batch_size = trial.suggest_int(\"batch_size\",8, 32, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    #max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 5.0) \n",
    "\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    config.dropout = dropout\n",
    "    config.attention_dropout = attention_dropout\n",
    "    config.num_labels = 2\n",
    "\n",
    "    # Split dataset into training and validation subsets (80-20)\n",
    "    \n",
    "    labels = tokenized_datasets['train']['label']\n",
    "    train_indices, val_indices = train_test_split(list(range(len(labels))), train_size=0.8, stratify=labels)\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"].select(train_indices)\n",
    "    val_dataset = tokenized_datasets[\"train\"].select(val_indices)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        weight_decay=weight_decay,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        #max_grad_norm=max_grad_norm,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # Define the Trainer using the above args\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Return the evaluation loss\n",
    "    return metrics[\"eval_loss\"]\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoConfig, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define search space\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.4)\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 5)\n",
    "    batch_size = trial.suggest_int(\"batch_size\",8, 32, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    #max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 5.0) \n",
    "\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    config.dropout = dropout\n",
    "    config.attention_dropout = attention_dropout\n",
    "    config.num_labels = 2\n",
    "\n",
    "    # Use KFold for cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    fold_losses = []\n",
    "\n",
    "    labels = tokenized_datasets['train']['label']\n",
    "    for train_idx, val_idx in kfold.split(tokenized_datasets[\"train\"][\"input_ids\"], labels):\n",
    "        train_subset = tokenized_datasets[\"train\"].select(train_idx)\n",
    "        val_subset = tokenized_datasets[\"train\"].select(val_idx)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=\"test_trainer\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            #max_grad_norm=max_grad_norm,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            load_best_model_at_end=True,\n",
    "        )\n",
    "\n",
    "        # Define the Trainer using the above args for the current fold\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_subset,\n",
    "            eval_dataset=val_subset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)],\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        fold_losses.append(metrics[\"eval_loss\"])\n",
    "\n",
    "    # Return the average loss across the folds\n",
    "    return sum(fold_losses) / len(fold_losses)\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "config.dropout = best_params['dropout']\n",
    "config.attention_dropout = best_params['attention_dropout']\n",
    "config.num_labels = 2 \n",
    "learning_rate = best_params[\"learning_rate\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "num_train_epochs = best_params['num_train_epochs']\n",
    "weight_decay = best_params['weight_decay']\n",
    "#max_grad_norm=best_params['max_grad_norm']\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on entire set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "full_dataset = concatenate_datasets([tokenized_datasets[\"train\"], tokenized_datasets[\"val\"]])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "     output_dir=\"test_trainer\",\n",
    "        #evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        #max_grad_norm=max_grad_norm,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=False,\n",
    "        evaluation_strategy='no', #uncomment if i want to train on train val split\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_dataset,\n",
    "    #train_dataset=tokenized_datasets[\"train\"],\n",
    "    #eval_dataset=tokenized_datasets[\"val\"],  # or you could leave this out if you just want to train on the full dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_clip = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predicted_labels = np.argmax(predictions_clip.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_clip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
